<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Evaluation of Interleaved Large Vision Language Models">
  <meta name="keywords" content="Interleaved Comprehension">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMIE</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png"> -->
  <link rel="icon" href="images/logo.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <script src="./static/js/leaderboard_testmini.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="color:brown; font-size: 70px;">
              <img id="painting_icon" width="15%" src="images/logo.png" style="vertical-align: bottom;"> MMIE
            </h1>
            <h3 class="title is-3 publication-title">Massive Multimodal Interleaved Comprehension Benchmark For Large
              Vision-Language Models</h3>
            <h5 class="subtitle is-5 publication-awards">ICLR 2025 Oral</h5>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://richard-peng-xia.github.io/" style="color:#f68946;font-weight:normal;">Peng
                  Xia<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://lillianwei-h.github.io/" style="color:#f68946;font-weight:normal;">Siwei
                  Han<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://stephenqsstarthomas.github.io/" style="color:#f68946;font-weight:normal;">Shi
                  Qiu<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://yiyangzhou.github.io/" style="color:#f68946;font-weight:normal;">Yiyang Zhou</a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#f68946;font-weight:normal;">Zhaoyang Wang</a>,
              </span>
              <span class="author-block">
                <a href="https://shenmishajing.github.io/" style="color:#f68946;font-weight:normal;">Wenhao Zheng</a>,
              </span>
              <span class="author-block">
                <a href="https://billchan226.github.io/" style="color:#f68946;font-weight:normal;">Zhaorun Chen</a>,
              </span>
              <span class="author-block">
                <a href="https://gzcch.github.io/" style="color:#f68946;font-weight:normal;">Chenhang Cui</a>,
              </span> <br>
              <span class="author-block">
                <a href="https://dingmyu.github.io/" style="color:#f68946;font-weight:normal;">Mingyu Ding</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ"
                  style="color:#f68946;font-weight:normal;">Linjie Li</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ"
                  style="color:#f68946;font-weight:normal;">Lijuan Wang</a>,
              </span>
              <span class="author-block">
                <a href="https://www.huaxiuyao.io/" style="color:#f68946;font-weight:normal;">Huaxiu Yao</a>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> UNC-Chapel
                Hill</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> University of
                Chicago</span>
              <span class="author-block"><b style="color:#0d760a; font-weight:normal">&#x25B6 </b> Microsoft
                Research</span>
              <span class="author-block"><b style="color:#eac035; font-weight:normal">&#x25B6 </b> NUS</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.10139" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://www.alphaxiv.org/abs/2410.10139" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>alphaXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/Lillianwei-h/MMIE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/MMIE/MMIE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/MMIE/MMIE-Score" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-robot"></i>
                    </span>
                    <span>Evaluation Model</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>




                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
          🔥<span style="color: #ff3860">[NEW!]</span> We introduce MMIE, a large-scale knowledge-intensive benchmark
          for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs)
          <!--           
          <br><br>
          🧐🔍 Findings: Models often show factual inaccuracies & fail to maintain fairness, also proving vulnerable to attacks with a lack of privacy awareness. -->
        </h4>
      </div>
    </div>
  </section>


  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <centering>
              <p>We present <strong>MMIE</strong>, a <strong>M</strong>assive <strong>M</strong>ultimodal
                <strong>I</strong>nterleaved understanding <strong>E</strong>valuation benchmark, designed for Large
                Vision-Language Models (LVLMs). MMIE offers a robust framework for evaluating the interleaved
                comprehension and generation capabilities of LVLMs across diverse fields, supported by reliable
                automated metrics.
              </p>
            </centering>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-six-fifths">
              <h3 class="title is-4">🌟 Key Features</h2>
            </div>
          </div>
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column is-full-width">
                <div class="content has-text-justified">
                  <br>
                  <div class="columns is-centered has-text-centered">
                    <h4><strong>🗂 Dataset</strong></h4>
                  </div>
                  <ul>
                    <li><strong>Comprehensive</strong>: 20K+ examples in interleaved multimodal format, consolidated into one
                      JSON file for easy access.</li>
                    <li><strong>Diverse</strong>: Spanning 12 fields and 102 subfields, offering broad and deep evaluation
                      across domains.</li>
                    <li><strong>Ground Truth Reference</strong>: Each question comes paired with a reference, ensuring
                      accurate
                      evaluations of model performance.</li>
                  </ul>
                  <br>
                  <div class="columns is-centered has-text-centered">
                    <h4><strong>⚙️ Metric</strong></h4>
                  </div>
                  <ul>
                    <li><strong>Automated Scoring</strong>: Evaluate your model’s results with our scoring model, <a
                        href="https://huggingface.co/MMIE/MMIE-Score" target="_blank">MMIE-Score</a>, powered by
                      <strong>InternVL-2-4B</strong>.
                    </li>
                    <li><strong>Bias Mitigation</strong>: Fine-tuned to reduce bias and ensure objective evaluations.</li>
                    <li><strong>Multimodal Capability</strong>: Tailored for interleaved inputs and outputs, evaluating both
                      text and image comprehension.</li>
                    <li><strong>High Correlation with Human Scores</strong>: Outperforms alternative metrics such as GPT-4o in
                      multimodal tasks, ensuring reliable benchmarking results.</li>
                  </ul>
      
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

    </div>
  </section>

  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="3%"
            src="images/logo.png"> MMIE Datasets</h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>MMIE is curated from four multimodal datasets, encompassing:</p>
            <ul>
              <li><strong>3 categories</strong>: Situational analysis, project-based learning, and multi-step reasoning.
              </li>
              <li><strong>12 fields</strong>: Mathematics, physics, coding, statistics, literature, philosophy,
                education, finance, health, sports, art, and Electrical Engineering and Computer Science (EECS).</li>
              <li><strong>102 subfields</strong>: Offering in-depth coverage across multiple domains.</li>
            </ul>
            <div style="text-align: center;">
              <img id="teaser" width="100%" src="images/overview.jpeg">
            </div>
            <br>
            <p>The dataset contains <strong>20,103 multimodal questions</strong> that support both interleaved inputs
              and outputs. It includes a mix of multiple-choice and open-ended questions, evaluating a wide range of
              competencies and reasoning skills. Each query is paired with a <strong>ground truth reference</strong>,
              enabling effective evaluation.</p>

            <p>In addition, we propose an <strong>automated evaluation metric</strong> powered by a scoring model, which
              is available for use at
              <a href="https://huggingface.co/MMIE/MMIE-Score">MMIE-Score</a>. This evaluation tool provides a streamlined
              way to assess your model's performance using the benchmark dataset.
            </p>


            <!-- CSS Code: Place this code in the document's head (between the 'head' tags) -->
            <style>
              table.GeneratedTable {
                width: 100%;
                background-color: #ffffff;
                border-collapse: collapse;
                border-width: 2px;
                border-color: #c1c4c5;
                border-style: solid;
                color: #000000;
              }

              table.GeneratedTable td,
              table.GeneratedTable th {
                border-width: 2px;
                border-color: #9b9d9e;
                border-style: solid;
                padding: 3px;
              }

              table.GeneratedTable thead {
                background-color: #6691ee;
              }
            </style>

            <!-- HTML Code: Place this code in the document's body (between the 'body' tags) where the table should appear -->
            <style>
              table.overveiw-table {
                border-collapse: collapse;
                width: 100%;
                font-family: Arial, sans-serif;
              }
            
              table.overveiw-table thead {
                background-color: #6b6c9b;
                color: white;
              }
            
              table.overveiw-table td {
                padding: 12px 15px;
                text-align: left;
                border: 1px solid #ddd;
              }
            
              table.overveiw-table tbody tr:nth-child(-n+5) {
                background-color: #ffffff;
              }

              table.overveiw-table tbody tr:nth-child(n+6):nth-child(-n+8) {
                background-color: #e0dfe769;
              }

              table.overveiw-table tbody tr:nth-child(n+9) {
                background-color: #ffffff;
              }
            
              table.overveiw-table tbody tr:hover {
                background-color: #ddd !important;
              }

              table.overveiw-table tbody tr td {
                font-size: 16px;
              }
            
              table.overveiw-table tbody tr td b {
                font-weight: bold;
              }
            </style>
            
            <div class="column is-six-fifths" width="80%">
              <table class="overveiw-table">
                <thead>
                  <tr>
                    <th style="color: #ffffff;">Statistic</th>
                    <th style="color: #ffffff;">Number</th>
                    <th style="color: #ffffff;">Percentage</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><b>Questions</b> </td>
                    <td>20103</td>
                    <td>-</td>
                  </tr>
                  <tr>
                    <td>- Situational analysis</td>
                    <td>5005</td>
                    <td>24.89%</td>
                  </tr>
                  <tr>
                    <td>- Project-based learning</td>
                    <td>11482</td>
                    <td>57.12%</td>
                  </tr>
                  <tr>
                    <td>- Multi-step reasoning</td>
                    <td>3616</td>
                    <td>17.99%</td>
                  </tr>
                  <tr>
                    <td><b>Total Categories/Fields/Subfields</b></td>
                    <td>3/12/102</td>
                    <td>-</td>
                  </tr>
                  <tr>
                    <td colspan="3"><b>Formats</b></td>
                  </tr>
                  <tr>
                    <td>- Multiple-Choice Questions</td>
                    <td>663</td>
                    <td>3.40%</td>
                  </tr>
                  <tr>
                    <td>- Open-Ended Questions</td>
                    <td>19340</td>
                    <td>96.60%</td>
                  </tr>
                  <tr>
                    <td><b>Questions with Images</b></td>
                    <td>20103</td>
                    <td>100%</td>
                  </tr>
                  <tr>
                    <td><b>Questions with answer label</b></td>
                    <td>20103</td>
                    <td>100%</td>
                  </tr>
                  <tr>
                    <td><b>Average question length</b></td>
                    <td>76.0</td>
                    <td>-</td>
                  </tr>
                  <tr>
                    <td><b>Average images per question</b></td>
                    <td>1.32</td>
                    <td>-</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <!-- Codes by Quackit.com -->

            </p>
          </div>
        </div>
      </div>


  </section>


  <!-- <section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="5%" src="images/logo.png"> CARES: A Benchmark of Trustworthiness in Medical Vision Language Models </h2>
    </div>
  </div>
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>
        CARES is designed to provide a comprehensive evaluation of trustworthiness in MedLVLMs, reflecting the issues present in model responses. We assess trustworthiness across five
        critical dimensions: trustfulness, fairness, safety, privacy, and robustness.
          <ul type="1">
            <li><b>Trustfulness</b>. <span style="font-size: 95%;">We discuss the trustfulness of MedLVLMs, defined as the extent to which a Med-LVLM
                can provide factual responses and recognize when
                those responses may potentially be incorrect.</span></li>
            <ul type="1">
              <li> <b>Factuality</b>: Med-LVLMs are susceptible to factual hallucination, wherein the model may generate incorrect or misleading information about medical conditions,
                  including erroneous judgments regarding symptoms or diseases, and inaccurate descriptions of medical images.
                  interventions.
              <li> <b>Uncertainty</b>: A trustful Med-LVLM should produce
                  confidence scores that accurately reflect the probability of its predictions being correct, essentially
                  offering precise uncertainty estimation. However, as various authors have noted, LLM-based models
                  often display overconfidence in their responses, which could potentially lead to a significant number
                  of misdiagnoses or erroneous diagnoses. </span></li>
            </ul>
            <li><b> Fairness</b>. <span style="font-size: 95%;">Med-LVLMs have the potential to unintentionally cause health disparities, especially among underrepresented groups. These disparities can reinforce stereotypes and lead to biased medical advice. It
                is essential to prioritize fairness in healthcare to guarantee that every individual receives equitable and
                accurate medical treatment.
          <li><b>Safety</b>. <span style="font-size: 95%;">Med-LVLMs present safety concerns, which include several aspects such as jailbreaking, overcautious behavior, and toxicity. </span></li>
          <ul type="1">
            <li> <b>Jailbreaking</b>: Jailbreaking refers to attempts or actions that
                manipulate or exploit a model to deviate from its intended
                functions or restrictions. For Med-LVLMs, it involves
                prompting the model in ways that allow access to restricted
                information or generating responses that violate medical guidelines.
            <li> <b>Overcautiousness</b>: Overcautiousness describes how
                Med-LVLMs often refrain from responding to medical queries they are capable of answering. In medical
                settings, this excessively cautious approach can lead
                models to decline answering common clinical diagnostic questions.
            <li> <b>Toxicity</b>: In Med-LVLMs, toxicity refers to outputs that are harmful, such as those containing biased,
                offensive, or inappropriate content. In medical applications, the impact of toxic outputs is
                particularly severe because they may generate rude or disrespectful medical advice, eroding trust in
                the application of clinical management. </span></li>
          </ul>
          <li><b> Privacy</b>. <span style="font-size: 95%;">Privacy breaches in Med-LVLMs is a critical issue due to the sensitive nature of health-related data.
              These models are expected to refrain from disclosing private information, such as marital status, as
              this can compromise both the reliability of the model and compliance with legal regulations.
          
          <li><b> Robustness </b>. <span style="font-size: 95%;"> Robustness in Med-LVLMs aims to evaluate whether the models perform reliably across various
              clinical settings. We focus on evaluating out-of-distribution (OOD) robustness, aiming
              to assess the model’s ability to handle test data whose distributions significantly differ from those
              of the training data.
          </ul>
        </p>
      </div>
    </div>
  </div>
</section> -->



  <section class="section">
    <!-- Results. -->
    
  </section>

  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">🔧 Benchmark Details</h2>
        <br>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="30%" src="images/data.png">
          <figcaption>
            Distribution of categories and fields in MMIE.
          </figcaption>
        </figure>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <br>
            <div class="columns is-centered has-text-centered">
              <h4><strong>🗂 Dataset</strong></h4>
            </div>
            <ul>
              MMIE evaluates LVLMs across interleaved multimodal comprehension and generation tasks. The dataset is
              carefully curated to ensure a wide range of examples across various fields, providing balanced coverage
              for comprehensive evaluations. These examples test reasoning, cognitive tasks, and multimodal alignment,
              ensuring detailed insights into model performance.
            </ul>
            <br>
            <div class="columns is-centered has-text-centered">
              <h4><strong>⚙️ Metric</strong></h4>
            </div>
            <div class="columns is-centered has-text-centered">
              <div class="column is-six-fifths"
                style="display: flex; align-items: flex-start; justify-content: center;">
                <figure style="text-align: center;">
                  <img id="teaser" width="30%"
                    src='https://cdn-uploads.huggingface.co/production/uploads/65941852f0152a21fc860f79/62MV7dB2_p2ptb2JXb6GH.png'>
                  <figcaption>
                    Pipeline of the scoring model.
                  </figcaption>
                </figure>
              </div>
            </div>
            <div>
              <p>The MMIE evaluation metric is built on <strong>InternVL-2-4B</strong>, a high-performing
                vision-language model fine-tuned for multimodal reasoning. This pipeline evaluates models including:</p>
              <ul>
                <li><strong>Text Quality</strong>: Clarity, coherence, and grammar.</li>
                <li><strong>Image Quality</strong>: Vividness and accuracy of image descriptions.</li>
                <li><strong>Text-Image Coherence</strong>: How well visual descriptions support the narrative.</li>
                <li><strong>Stylistic Consistency</strong>: Consistent style and structure throughout text and images.
                </li>
                <li>For detailed evaluation criteria, please refer to Appendix A.9 in our paper</li>
                <li>
                  <div align="center">
                    <img
                      src='https://cdn-uploads.huggingface.co/production/uploads/65941852f0152a21fc860f79/YmDZxBR7OtWra5F016igi.png'
                      width=70%>
                  </div>
                  <p><em>Note: Higher values indicate better performance for Pearson and Cosine Similarity, while lower
                      values are better for MSE and MAE.</em></p>

                  <p>The MMIE evaluation metric achieves high correlations with human annotations in all aspects of
                    multimodal comprehension and generation. It consistently outperforms other metrics, like GPT-4o,
                    making it ideal for large-scale model benchmarking and comparison.</p>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section">

<div class="container mt-5">
    <div class="form-row" style="justify-content: flex-end;">
      <div class="form-group col-md-1">
        <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
        <div class="btn-group" role="group" aria-label="Left and Right Controller"
          style="width: 100%;align-items: flex-end;justify-content: center;flex-direction: row;display: flex;">
          <button type="button" class="form-control btn btn-primary" id="prev-question"><i
              class="material-icons">keyboard_arrow_left</i></button>
          <button type="button" class="form-control btn btn-primary" id="next-question"><i
              class="material-icons">keyboard_arrow_right</i></button>
        </div>
      </div>
    </div> -->

  <!-- Question Card -->
  <!-- <div style="display: flex; justify-content: center; align-items: center;">
      <div class="card mb-4" style="width: 60%; display: flex; align-items: center;">
        <div class="card-body" id="selected-question" style="display: flex; height: 50vh;">
          <div class="chat-history">
          </div>

        </div>
      </div>
    </div>

  </div>

</section> -->

  <section class="section">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-full has-text-centered content">

          <h2 class="title is-3" id="leaderboard">🏆 Leaderboard</h2>
          <div class="content">
            <p class="mt-3">
              MMIE provides a systematic evaluation of existing open-source LVLMs supporting interleaved multimodal input and output <b>interleaved LVLMs</b>, along with the integration of state-of-the-art LVLMs and text-to-image generative models <b>integrated LVLMs</b>. To view detailed results, please see the paper. Leaderboard is also available on <a
                href='https://huggingface.co/spaces/MMIE/Leaderboard'>huggingface</a>.
            </p>
            <p>
              Scores on MMIE benchmark.
            </p>
            <table class="js-sort-table" id="results" style="margin-left: auto; margin-right: auto;">
              <thead>
                <tr>
                  <th style="vertical-align: middle; width: 180px;"><strong>Model</strong></th>
                  <th style="vertical-align: middle; width: 180px;"><strong>Model Type</strong></th>
                  <th style="vertical-align: middle; width: 180px;"><strong>Situational analysis</strong></th>
                  <th style="vertical-align: middle; width: 180px;"><strong>Project-based learning</strong></th>
                  <th style="vertical-align: middle; width: 180px;"><strong>Multi-step reasoning</strong></th>
                  <th style="vertical-align: middle; width: 180px;"><strong>AVG</strong></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>MiniGPT-5</td>
                  <td>Interleaved LVLM</td>
                  <td>47.63</td>
                  <td>55.12</td>
                  <td>42.17</td>
                  <td>50.92</td>
                </tr>
                <tr>
                  <td>EMU-2</td>
                  <td>Interleaved LVLM</td>
                  <td>39.65</td>
                  <td>46.12</td>
                  <td>50.75</td>
                  <td>45.33</td>
                </tr>
                <tr>
                  <td>GILL</td>
                  <td>Interleaved LVLM</td>
                  <td>46.72</td>
                  <td>57.57</td>
                  <td>39.33</td>
                  <td>51.58</td>
                </tr>
                <tr>
                  <td>Anole</td>
                  <td>Interleaved LVLM</td>
                  <td>48.95</td>
                  <td>59.05</td>
                  <td>51.72</td>
                  <td>55.22</td>
                </tr>
                <tr>
                  <td>GPT-4o | Openjourney</td>
                  <td>Integrated LVLM</td>
                  <td>53.05</td>
                  <td>71.4</td>
                  <td>53.67</td>
                  <td>63.65</td>
                </tr>
                <tr>
                  <td>GPT-4o | SD-3</td>
                  <td>Integrated LVLM</td>
                  <td>53</td>
                  <td>71.2</td>
                  <td>53.67</td>
                  <td>63.52</td>
                </tr>
                <tr>
                  <td>GPT-4o | SD-XL</td>
                  <td>Integrated LVLM</td>
                  <td>56.12</td>
                  <td>73.25</td>
                  <td>53.67</td>
                  <td>65.47</td>
                </tr>
                <tr>
                  <td>GPT-4o | Flux</td>
                  <td>Integrated LVLM</td>
                  <td>54.97</td>
                  <td>68.8</td>
                  <td>53.67</td>
                  <td>62.63</td>
                </tr>
                <tr>
                  <td>Gemini-1.5 | Openjourney</td>
                  <td>Integrated LVLM</td>
                  <td>48.08</td>
                  <td>67.93</td>
                  <td>60.05</td>
                  <td>61.57</td>
                </tr>
                <tr>
                  <td>Gemini-1.5 | SD-3</td>
                  <td>Integrated LVLM</td>
                  <td>47.48</td>
                  <td>68.7</td>
                  <td>60.05</td>
                  <td>61.87</td>
                </tr>
                <tr>
                  <td>Gemini-1.5 | SD-XL</td>
                  <td>Integrated LVLM</td>
                  <td>49.43</td>
                  <td>71.85</td>
                  <td>60.05</td>
                  <td>64.15</td>
                </tr>
                <tr>
                  <td>Gemini-1.5 | Flux</td>
                  <td>Integrated LVLM</td>
                  <td>47.07</td>
                  <td>68.33</td>
                  <td>60.05</td>
                  <td>61.55</td>
                </tr>
                <tr>
                  <td>LLAVA-34b | Openjourney</td>
                  <td>Integrated LVLM</td>
                  <td>54.12</td>
                  <td>73.47</td>
                  <td>47.28</td>
                  <td>63.93</td>
                </tr>
                <tr>
                  <td>LLAVA-34b | SD-3</td>
                  <td>Integrated LVLM</td>
                  <td>54.72</td>
                  <td>72.55</td>
                  <td>47.28</td>
                  <td>63.57</td>
                </tr>
                <tr>
                  <td>LLAVA-34b | SD-XL</td>
                  <td>Integrated LVLM</td>
                  <td>55.97</td>
                  <td>74.6</td>
                  <td>47.28</td>
                  <td>65.05</td>
                </tr>
                <tr>
                  <td>LLAVA-34b | Flux</td>
                  <td>Integrated LVLM</td>
                  <td>54.23</td>
                  <td>71.32</td>
                  <td>47.28</td>
                  <td>62.73</td>
                </tr>
                <tr>
                  <td>Qwen-VL-70b | Openjourney</td>
                  <td>Integrated LVLM</td>
                  <td>52.73</td>
                  <td>71.63</td>
                  <td>55.63</td>
                  <td>64.05</td>
                </tr>
                <tr>
                  <td>Qwen-VL-70b | SD-3</td>
                  <td>Integrated LVLM</td>
                  <td>54.98</td>
                  <td>71.87</td>
                  <td>55.63</td>
                  <td>64.75</td>
                </tr>
                <tr>
                  <td>Qwen-VL-70b | SD-XL</td>
                  <td>Integrated LVLM</td>
                  <td>52.58</td>
                  <td>73.57</td>
                  <td>55.63</td>
                  <td>65.12</td>
                </tr>
                <tr>
                  <td>Qwen-VL-70b | Flux</td>
                  <td>Integrated LVLM</td>
                  <td>54.23</td>
                  <td>69.47</td>
                  <td>55.63</td>
                  <td>63.18</td>
                </tr>
              </tbody>
            </table>
          </div>

        </div>
      </div>

    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{xia2024mmie,
  title={MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models},
  author={Xia, Peng and Han, Siwei and Qiu, Shi and Zhou, Yiyang and Wang, Zhaoyang and Zheng, Wenhao and Chen, Zhaorun and Cui, Chenhang and Ding, Mingyu and Li, Linjie and Wang, Lijuan and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2410.10139},
  year={2024}
}
  </code></pre>
    </div>
  </section>

  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under
        a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        We would like to express our sincere gratitude to the teams behind <a href="https://internvl.readthedocs.io/en/latest/"> InternVL</a>, <a href="https://eric-ai-lab.github.io/minigpt-5.github.io/">MiniGPT</a>, <a href="https://baaivision.github.io/emu2/">EMU</a>, <a href="https://jykoh.com/gill">GILL</a>, <a href="https://gair-nlp.github.io/anole/">Anole</a>, <a href="https://llava-vl.github.io">LLaVA</a>, <a href="https://qwen2.org/vl/">Qwen2-VL</a>, <a href="https://openjourney.art">Openjourney</a>, <a href="https://stability.ai/news/stable-diffusion-3-medium">Stable Diffusion</a> and <a href="https://blackforestlabs.ai">Flux</a> for providing open-source models.
      </p>
      <a href="https://opensource.org/license/mit" target="_blank">
        <img src="https://img.shields.io/badge/License-MIT-white.svg" alt="License: MIT">
      </a>
      <!-- <p> -->
      <!-- <b>Usage and License Notices</b>: The data and code is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of CLIP,  LLaMA, Vicuna, GPT-4, LLaVA. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. -->
      <!-- </p>

      <p>
      <a href='https://github.com/Computer-Vision-in-the-Wild/'><img id="painting_icon" width="10%" src="https://avatars.githubusercontent.com/u/97258247?s=200&v=4"></a>  -->
      <!--      Related Links: -->
      <!--      <a href='https://react-vl.github.io/'>[REACT]</a>  -->
      <!--      <a href='https://gligen.github.io/'>[GLIGEN]</a> -->
      <!--      <a href='https://github.com/Computer-Vision-in-the-Wild/'>[Computer Vision in the Wild (CVinW)]</a> -->
      <!--      <a href='https://instruction-tuning-with-gpt-4.github.io/'>[Insutrction Tuning with GPT-4]</a>      -->
      </p>
    </div>
  </section>

  <script>
    // Handle message showing
    function createChatRow(sender, text, imageSrc) {
      var article = document.createElement("article");
      article.className = "media"

      var figure = document.createElement("figure");
      figure.className = "media-left";

      var span = document.createElement("span");
      span.className = "icon is-large";

      var icon = document.createElement("i");
      icon.className = "fas fas fa-2x" + (sender === "User" ? " fa-user " : sender === "LLaVA" ? " fa-robot" : "");

      var media = document.createElement("div");
      media.className = "media-content";

      var content = document.createElement("div");
      content.className = "content";

      var para = document.createElement("p");

      // wrap text in pre tag to preserve whitespace and line breaks
      var pre_text = document.createElement("pre");
      pre_text.style = "background-color: white; font-size: 18px; font-family: Arial; padding: 0; margin: 0; white-space: pre-wrap; word-wrap: break-word;";
      var paraText = document.createTextNode(text);
      pre_text.appendChild(paraText);

      var strong = document.createElement("strong");
      strong.innerHTML = sender;
      var br = document.createElement("br");

      para.appendChild(strong);
      para.appendChild(br);
      para.appendChild(pre_text);

      // Add image if imageSrc is provided
      if (imageSrc) {
        var img = document.createElement("img");
        img.src = imageSrc;
        img.style = "max-width: 100%; max-height: 300px;"; // Adjust the style as needed
        para.appendChild(img);
      }

      content.appendChild(para);
      media.appendChild(content);
      span.appendChild(icon);
      figure.appendChild(span);
      if (sender !== "Description") {
        article.appendChild(figure);
      };
      article.appendChild(media);
      return article;
    }

    function addMessageToChatHistory(sender, message, imageSrc) {
      const chatHistory = document.querySelector('.chat-history');
      const chatRow = createChatRow(sender, message, imageSrc);
      chatHistory.appendChild(chatRow);
      chatHistory.scrollTop = chatHistory.scrollHeight;
    }

    function clearChatHistory() {
      const chatHistory = document.querySelector('.chat-history');
      chatHistory.innerHTML = "";
    }

    // 
    const conversations = [
      {
        "turns": [
          ["User", "", "images/result1.png"],
        ]
      },
      {
        "turns": [
          ["User", "", "images/result2.png"]]
      },
      {
        "turns": [
          ["User", "", "images/result3.png"]]
      },
      {
        "turns": [
          ["User", "", "images/result4.png"],
        ]
      },
      {
        "turns": [
          ["User", "", "images/result5.png"]]
      },
      {
        "turns": [
          ["User", "", "images/result6.png"]]
      },
      {
        "turns": [
          ["User", "", "images/result7.png"]]
      },
      {
        "turns": [
          ["User", "", "images/result8.png"]]
      },
    ];

    // The current image index
    let currentIndex = 0;

    // The function to update the displayed chat history
    function update_dialog_demo() {
      // Clear the chat history
      clearChatHistory();

      for (let i = 0; i < conversations[currentIndex].turns.length; i++) {
        if (conversations[currentIndex].turns[i].length == 2) {
          addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1]);
        }
        else {
          addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1], conversations[currentIndex].turns[i][2]);
        }
      }

      // scroll to the top of the chat history
      document.querySelector('.chat-history').scrollTop = 0;
    }

    // Initialize the displayed image
    update_dialog_demo();

    // Event listeners for the buttons
    document.getElementById('prev-question').addEventListener('click', () => {
      currentIndex = (currentIndex - 1 + conversations.length) % conversations.length;
      update_dialog_demo();
    });

    document.getElementById('next-question').addEventListener('click', () => {
      currentIndex = (currentIndex + 1) % conversations.length;
      update_dialog_demo();
    });


  </script>

</body>

</html>
